{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e990348",
   "metadata": {},
   "source": [
    "# Training a video classification model\n",
    "\n",
    "\n",
    "### Step 1: Dataset\n",
    " 1. Save data in a more efficient format for reading\n",
    " 2. Write a Dataset for the converted videos. Given an index, return a N frame 'clip' from the video\n",
    " \n",
    " i.e. \n",
    " \n",
    "Video1 is 25 frames, Video2 is 100 frames. N = 10. \n",
    "\n",
    "\n",
    "Index 3 gives Video1 frames [3-13)\n",
    "\n",
    "Index 15 gives Video1 frames [15-25)\n",
    "\n",
    "Index 16 gives Video2 frames [0-10)\n",
    "\n",
    "A useful formula: The number of N frame 'clips' in a M frame video = M-N+1\n",
    "\n",
    "### Step 2: Model\n",
    "\n",
    "### Step 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcf4cd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "import torchvision\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from torchsummary import summary # pip install torchsummary\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e940aa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_videos = r\"C:\\Users\\willc\\Downloads\\val_rgb_front_clips\\raw_videos\"\n",
    "path_to_new_videos = r\"C:\\Users\\willc\\Downloads\\val_rgb_front_clips\\processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11efd8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore this cell. It is useful operations that are not available in all versions of PyTorch\n",
    "from torchvision.transforms import RandomResizedCrop\n",
    "def _is_tensor_video_clip(clip):\n",
    "    if not torch.is_tensor(clip):\n",
    "        raise TypeError(\"clip should be Tesnor. Got %s\" % type(clip))\n",
    "\n",
    "    if not clip.ndimension() == 4:\n",
    "        raise ValueError(\"clip should be 4D. Got %dD\" % clip.dim())\n",
    "\n",
    "    return True\n",
    "def crop(clip, i, j, h, w):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        clip (torch.tensor): Video clip to be cropped. Size is (C, T, H, W)\n",
    "    \"\"\"\n",
    "    assert len(clip.size()) == 4, \"clip should be a 4D tensor\"\n",
    "    return clip[..., i:i + h, j:j + w]\n",
    "\n",
    "\n",
    "def resize(clip, target_size, interpolation_mode):\n",
    "    assert len(target_size) == 2, \"target size should be tuple (height, width)\"\n",
    "    return torch.nn.functional.interpolate(\n",
    "        clip, size=target_size, mode=interpolation_mode\n",
    "    )\n",
    "def resized_crop(clip, i, j, h, w, size, interpolation_mode=\"bilinear\"):\n",
    "    \"\"\"\n",
    "    Do spatial cropping and resizing to the video clip\n",
    "    Args:\n",
    "        clip (torch.tensor): Video clip to be cropped. Size is (C, T, H, W)\n",
    "        i (int): i in (i,j) i.e coordinates of the upper left corner.\n",
    "        j (int): j in (i,j) i.e coordinates of the upper left corner.\n",
    "        h (int): Height of the cropped region.\n",
    "        w (int): Width of the cropped region.\n",
    "        size (tuple(int, int)): height and width of resized clip\n",
    "    Returns:\n",
    "        clip (torch.tensor): Resized and cropped clip. Size is (C, T, H, W)\n",
    "    \"\"\"\n",
    "    assert _is_tensor_video_clip(clip), \"clip should be a 4D torch.tensor\"\n",
    "    clip = crop(clip, i, j, h, w)\n",
    "    print(clip.shape)\n",
    "    clip = resize(clip, size, interpolation_mode)\n",
    "    print(clip.shape)\n",
    "    return clip\n",
    "def normalize(clip, mean, std, inplace=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        clip (torch.tensor): Video clip to be normalized. Size is (C, T, H, W)\n",
    "        mean (tuple): pixel RGB mean. Size is (3)\n",
    "        std (tuple): pixel standard deviation. Size is (3)\n",
    "    Returns:\n",
    "        normalized clip (torch.tensor): Size is (C, T, H, W)\n",
    "    \"\"\"\n",
    "    assert _is_tensor_video_clip(clip), \"clip should be a 4D torch.tensor\"\n",
    "    if not inplace:\n",
    "        clip = clip.clone()\n",
    "    mean = torch.as_tensor(mean, dtype=clip.dtype, device=clip.device)\n",
    "    std = torch.as_tensor(std, dtype=clip.dtype, device=clip.device)\n",
    "    clip.sub_(mean[:, None, None, None]).div_(std[:, None, None, None])\n",
    "    return clip\n",
    "class RandomResizedCropVideo(RandomResizedCrop):\n",
    "    def __init__(\n",
    "        self,\n",
    "        size,\n",
    "        scale=(0.08, 1.0),\n",
    "        ratio=(3.0 / 4.0, 4.0 / 3.0),\n",
    "        interpolation_mode=\"bilinear\",\n",
    "    ):\n",
    "        if isinstance(size, tuple):\n",
    "            assert len(size) == 2, \"size should be tuple (height, width)\"\n",
    "            self.size = size\n",
    "        else:\n",
    "            self.size = (size, size)\n",
    "\n",
    "        self.interpolation_mode = interpolation_mode\n",
    "        self.scale = scale\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            clip (torch.tensor): Video clip to be cropped. Size is (C, T, H, W)\n",
    "        Returns:\n",
    "            torch.tensor: randomly cropped/resized video clip.\n",
    "                size is (C, T, H, W)\n",
    "        \"\"\"\n",
    "        i, j, h, w = self.get_params(clip, self.scale, self.ratio)\n",
    "        j = 280  # center the crop\n",
    "        return resized_crop(clip, i, j, h, w, self.size, self.interpolation_mode)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \\\n",
    "            '(size={0}, interpolation_mode={1}, scale={2}, ratio={3})'.format(\n",
    "                self.size, self.interpolation_mode, self.scale, self.ratio\n",
    "            )\n",
    "class NormalizeVideo(object):\n",
    "    \"\"\"\n",
    "    Normalize the video clip by mean subtraction and division by standard deviation\n",
    "    Args:\n",
    "        mean (3-tuple): pixel RGB mean\n",
    "        std (3-tuple): pixel RGB standard deviation\n",
    "        inplace (boolean): whether do in-place normalization\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std, inplace=False):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            clip (torch.tensor): video clip to be normalized. Size is (C, T, H, W)\n",
    "        \"\"\"\n",
    "        return normalize(clip, self.mean, self.std, self.inplace)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1}, inplace={2})'.format(\n",
    "            self.mean, self.std, self.inplace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "424b7ac1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "PyAV is not installed, and is necessary for the video operations in torchvision.\nSee https://github.com/mikeboers/PyAV#installation for instructions on how to\ninstall PyAV on your system.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mwillc\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mval_rgb_front_clips\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mraw_videos\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tmp\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\.conda\\envs\\496\\lib\\site-packages\\torchvision\\io\\video.py:273\u001b[0m, in \u001b[0;36mread_video\u001b[1;34m(filename, start_pts, end_pts, pts_unit)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_video_backend() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyav\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _video_opt\u001b[38;5;241m.\u001b[39m_read_video(filename, start_pts, end_pts, pts_unit)\n\u001b[1;32m--> 273\u001b[0m \u001b[43m_check_av_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end_pts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    276\u001b[0m     end_pts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\496\\lib\\site-packages\\torchvision\\io\\video.py:42\u001b[0m, in \u001b[0;36m_check_av_available\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_av_available\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(av, \u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[1;32m---> 42\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m av\n",
      "\u001b[1;31mImportError\u001b[0m: PyAV is not installed, and is necessary for the video operations in torchvision.\nSee https://github.com/mikeboers/PyAV#installation for instructions on how to\ninstall PyAV on your system.\n"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "for file in glob.glob(r\"C:\\Users\\willc\\Downloads\\val_rgb_front_clips\\raw_videos\\*\"):\n",
    "    tmp = torchvision.io.read_video(file)\n",
    "    print(tmp.shape)\n",
    "#     frames.append(tmp.shape[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc345aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(frames, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e6e4df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c14c1cd703774368ae6a26c3505b3a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 189, 720, 720])\n",
      "torch.Size([3, 189, 224, 224])\n",
      "torch.Size([3, 32, 720, 720])\n",
      "torch.Size([3, 32, 224, 224])\n",
      "torch.Size([3, 172, 720, 720])\n",
      "torch.Size([3, 172, 224, 224])\n",
      "torch.Size([3, 42, 720, 720])\n",
      "torch.Size([3, 42, 224, 224])\n",
      "torch.Size([3, 82, 720, 720])\n",
      "torch.Size([3, 82, 224, 224])\n",
      "torch.Size([3, 140, 720, 720])\n",
      "torch.Size([3, 140, 224, 224])\n",
      "torch.Size([3, 111, 720, 720])\n",
      "torch.Size([3, 111, 224, 224])\n",
      "torch.Size([3, 43, 720, 720])\n",
      "torch.Size([3, 43, 224, 224])\n",
      "torch.Size([3, 199, 720, 720])\n",
      "torch.Size([3, 199, 224, 224])\n",
      "torch.Size([3, 174, 720, 720])\n",
      "torch.Size([3, 174, 224, 224])\n",
      "torch.Size([3, 89, 720, 720])\n",
      "torch.Size([3, 89, 224, 224])\n",
      "torch.Size([3, 68, 720, 720])\n",
      "torch.Size([3, 68, 224, 224])\n",
      "torch.Size([3, 165, 720, 720])\n",
      "torch.Size([3, 165, 224, 224])\n",
      "torch.Size([3, 63, 720, 720])\n",
      "torch.Size([3, 63, 224, 224])\n",
      "torch.Size([3, 100, 720, 720])\n",
      "torch.Size([3, 100, 224, 224])\n",
      "torch.Size([3, 90, 720, 720])\n",
      "torch.Size([3, 90, 224, 224])\n",
      "torch.Size([3, 120, 720, 720])\n",
      "torch.Size([3, 120, 224, 224])\n",
      "torch.Size([3, 74, 720, 720])\n",
      "torch.Size([3, 74, 224, 224])\n",
      "torch.Size([3, 104, 720, 720])\n",
      "torch.Size([3, 104, 224, 224])\n",
      "torch.Size([3, 66, 720, 720])\n",
      "torch.Size([3, 66, 224, 224])\n",
      "torch.Size([3, 78, 720, 720])\n",
      "torch.Size([3, 78, 224, 224])\n",
      "torch.Size([3, 138, 720, 720])\n",
      "torch.Size([3, 138, 224, 224])\n",
      "torch.Size([3, 82, 720, 720])\n",
      "torch.Size([3, 82, 224, 224])\n",
      "torch.Size([3, 228, 720, 720])\n",
      "torch.Size([3, 228, 224, 224])\n",
      "torch.Size([3, 58, 720, 720])\n",
      "torch.Size([3, 58, 224, 224])\n",
      "torch.Size([3, 36, 720, 720])\n",
      "torch.Size([3, 36, 224, 224])\n",
      "torch.Size([3, 164, 720, 720])\n",
      "torch.Size([3, 164, 224, 224])\n",
      "torch.Size([3, 319, 720, 720])\n",
      "torch.Size([3, 319, 224, 224])\n",
      "torch.Size([3, 88, 720, 720])\n",
      "torch.Size([3, 88, 224, 224])\n",
      "torch.Size([3, 85, 720, 720])\n",
      "torch.Size([3, 85, 224, 224])\n",
      "torch.Size([3, 89, 720, 720])\n",
      "torch.Size([3, 89, 224, 224])\n",
      "torch.Size([3, 353, 720, 720])\n",
      "torch.Size([3, 353, 224, 224])\n",
      "torch.Size([3, 91, 720, 720])\n",
      "torch.Size([3, 91, 224, 224])\n",
      "torch.Size([3, 79, 720, 720])\n",
      "torch.Size([3, 79, 224, 224])\n",
      "torch.Size([3, 65, 720, 720])\n",
      "torch.Size([3, 65, 224, 224])\n",
      "torch.Size([3, 79, 720, 720])\n",
      "torch.Size([3, 79, 224, 224])\n",
      "torch.Size([3, 116, 720, 720])\n",
      "torch.Size([3, 116, 224, 224])\n",
      "torch.Size([3, 89, 720, 720])\n",
      "torch.Size([3, 89, 224, 224])\n",
      "torch.Size([3, 153, 720, 720])\n",
      "torch.Size([3, 153, 224, 224])\n",
      "torch.Size([3, 49, 720, 720])\n",
      "torch.Size([3, 49, 224, 224])\n",
      "torch.Size([3, 63, 720, 720])\n",
      "torch.Size([3, 63, 224, 224])\n",
      "torch.Size([3, 256, 720, 720])\n",
      "torch.Size([3, 256, 224, 224])\n",
      "torch.Size([3, 122, 720, 720])\n",
      "torch.Size([3, 122, 224, 224])\n",
      "torch.Size([3, 230, 720, 720])\n",
      "torch.Size([3, 230, 224, 224])\n",
      "torch.Size([3, 106, 720, 720])\n",
      "torch.Size([3, 106, 224, 224])\n",
      "torch.Size([3, 187, 720, 720])\n",
      "torch.Size([3, 187, 224, 224])\n",
      "torch.Size([3, 45, 720, 720])\n",
      "torch.Size([3, 45, 224, 224])\n",
      "torch.Size([3, 124, 720, 720])\n",
      "torch.Size([3, 124, 224, 224])\n",
      "torch.Size([3, 59, 720, 720])\n",
      "torch.Size([3, 59, 224, 224])\n",
      "torch.Size([3, 28, 720, 720])\n",
      "torch.Size([3, 28, 224, 224])\n",
      "torch.Size([3, 38, 720, 720])\n",
      "torch.Size([3, 38, 224, 224])\n",
      "torch.Size([3, 227, 720, 720])\n",
      "torch.Size([3, 227, 224, 224])\n",
      "torch.Size([3, 638, 720, 720])\n",
      "torch.Size([3, 638, 224, 224])\n",
      "torch.Size([3, 125, 720, 720])\n",
      "torch.Size([3, 125, 224, 224])\n",
      "torch.Size([3, 162, 720, 720])\n",
      "torch.Size([3, 162, 224, 224])\n",
      "torch.Size([3, 411, 720, 720])\n",
      "torch.Size([3, 411, 224, 224])\n",
      "torch.Size([3, 291, 720, 720])\n",
      "torch.Size([3, 291, 224, 224])\n",
      "torch.Size([3, 101, 720, 720])\n",
      "torch.Size([3, 101, 224, 224])\n",
      "torch.Size([3, 35, 720, 720])\n",
      "torch.Size([3, 35, 224, 224])\n",
      "torch.Size([3, 32, 720, 720])\n",
      "torch.Size([3, 32, 224, 224])\n",
      "torch.Size([3, 195, 720, 720])\n",
      "torch.Size([3, 195, 224, 224])\n",
      "torch.Size([3, 31, 720, 720])\n",
      "torch.Size([3, 31, 224, 224])\n",
      "torch.Size([3, 85, 720, 720])\n",
      "torch.Size([3, 85, 224, 224])\n",
      "torch.Size([3, 26, 720, 720])\n",
      "torch.Size([3, 26, 224, 224])\n",
      "torch.Size([3, 208, 720, 720])\n",
      "torch.Size([3, 208, 224, 224])\n",
      "torch.Size([3, 8, 720, 720])\n",
      "torch.Size([3, 8, 224, 224])\n",
      "torch.Size([3, 117, 720, 720])\n",
      "torch.Size([3, 117, 224, 224])\n",
      "torch.Size([3, 62, 720, 720])\n",
      "torch.Size([3, 62, 224, 224])\n",
      "torch.Size([3, 16, 720, 720])\n",
      "torch.Size([3, 16, 224, 224])\n",
      "torch.Size([3, 21, 720, 720])\n",
      "torch.Size([3, 21, 224, 224])\n",
      "torch.Size([3, 203, 720, 720])\n",
      "torch.Size([3, 203, 224, 224])\n",
      "torch.Size([3, 151, 720, 720])\n",
      "torch.Size([3, 151, 224, 224])\n",
      "torch.Size([3, 179, 720, 720])\n",
      "torch.Size([3, 179, 224, 224])\n",
      "torch.Size([3, 45, 720, 720])\n",
      "torch.Size([3, 45, 224, 224])\n",
      "torch.Size([3, 26, 720, 720])\n",
      "torch.Size([3, 26, 224, 224])\n",
      "torch.Size([3, 70, 720, 720])\n",
      "torch.Size([3, 70, 224, 224])\n",
      "torch.Size([3, 34, 720, 720])\n",
      "torch.Size([3, 34, 224, 224])\n",
      "torch.Size([3, 8, 720, 720])\n",
      "torch.Size([3, 8, 224, 224])\n",
      "torch.Size([3, 170, 720, 720])\n",
      "torch.Size([3, 170, 224, 224])\n",
      "torch.Size([3, 6, 720, 720])\n",
      "torch.Size([3, 6, 224, 224])\n",
      "torch.Size([3, 60, 720, 720])\n",
      "torch.Size([3, 60, 224, 224])\n",
      "torch.Size([3, 79, 720, 720])\n",
      "torch.Size([3, 79, 224, 224])\n",
      "torch.Size([3, 73, 720, 720])\n",
      "torch.Size([3, 73, 224, 224])\n",
      "torch.Size([3, 93, 720, 720])\n",
      "torch.Size([3, 93, 224, 224])\n",
      "torch.Size([3, 40, 720, 720])\n",
      "torch.Size([3, 40, 224, 224])\n",
      "torch.Size([3, 314, 720, 720])\n",
      "torch.Size([3, 314, 224, 224])\n",
      "torch.Size([3, 314, 720, 720])\n",
      "torch.Size([3, 314, 224, 224])\n",
      "torch.Size([3, 505, 720, 720])\n",
      "torch.Size([3, 505, 224, 224])\n",
      "torch.Size([3, 505, 720, 720])\n",
      "torch.Size([3, 505, 224, 224])\n",
      "torch.Size([3, 296, 720, 720])\n",
      "torch.Size([3, 296, 224, 224])\n",
      "torch.Size([3, 296, 720, 720])\n",
      "torch.Size([3, 296, 224, 224])\n",
      "torch.Size([3, 20, 720, 720])\n",
      "torch.Size([3, 20, 224, 224])\n",
      "torch.Size([3, 20, 720, 720])\n",
      "torch.Size([3, 20, 224, 224])\n",
      "torch.Size([3, 114, 720, 720])\n",
      "torch.Size([3, 114, 224, 224])\n",
      "torch.Size([3, 114, 720, 720])\n",
      "torch.Size([3, 114, 224, 224])\n",
      "torch.Size([3, 148, 720, 720])\n",
      "torch.Size([3, 148, 224, 224])\n",
      "torch.Size([3, 148, 720, 720])\n",
      "torch.Size([3, 148, 224, 224])\n",
      "torch.Size([3, 34, 720, 720])\n",
      "torch.Size([3, 34, 224, 224])\n",
      "torch.Size([3, 34, 720, 720])\n",
      "torch.Size([3, 34, 224, 224])\n",
      "torch.Size([3, 133, 720, 720])\n",
      "torch.Size([3, 133, 224, 224])\n",
      "torch.Size([3, 133, 720, 720])\n",
      "torch.Size([3, 133, 224, 224])\n",
      "torch.Size([3, 38, 720, 720])\n",
      "torch.Size([3, 38, 224, 224])\n",
      "torch.Size([3, 38, 720, 720])\n",
      "torch.Size([3, 38, 224, 224])\n",
      "torch.Size([3, 110, 720, 720])\n",
      "torch.Size([3, 110, 224, 224])\n",
      "torch.Size([3, 110, 720, 720])\n",
      "torch.Size([3, 110, 224, 224])\n",
      "torch.Size([3, 167, 720, 720])\n",
      "torch.Size([3, 167, 224, 224])\n",
      "torch.Size([3, 167, 720, 720])\n",
      "torch.Size([3, 167, 224, 224])\n",
      "torch.Size([3, 188, 720, 720])\n",
      "torch.Size([3, 188, 224, 224])\n",
      "torch.Size([3, 188, 720, 720])\n",
      "torch.Size([3, 188, 224, 224])\n",
      "torch.Size([3, 115, 720, 720])\n",
      "torch.Size([3, 115, 224, 224])\n",
      "torch.Size([3, 115, 720, 720])\n",
      "torch.Size([3, 115, 224, 224])\n",
      "torch.Size([3, 52, 720, 720])\n",
      "torch.Size([3, 52, 224, 224])\n",
      "torch.Size([3, 52, 720, 720])\n",
      "torch.Size([3, 52, 224, 224])\n",
      "torch.Size([3, 37, 720, 720])\n",
      "torch.Size([3, 37, 224, 224])\n",
      "torch.Size([3, 37, 720, 720])\n",
      "torch.Size([3, 37, 224, 224])\n",
      "torch.Size([3, 32, 720, 720])\n",
      "torch.Size([3, 32, 224, 224])\n",
      "torch.Size([3, 32, 720, 720])\n",
      "torch.Size([3, 32, 224, 224])\n",
      "torch.Size([3, 140, 720, 720])\n",
      "torch.Size([3, 140, 224, 224])\n",
      "torch.Size([3, 140, 720, 720])\n",
      "torch.Size([3, 140, 224, 224])\n",
      "torch.Size([3, 141, 720, 720])\n",
      "torch.Size([3, 141, 224, 224])\n",
      "torch.Size([3, 141, 720, 720])\n",
      "torch.Size([3, 141, 224, 224])\n",
      "torch.Size([3, 72, 720, 720])\n",
      "torch.Size([3, 72, 224, 224])\n",
      "torch.Size([3, 72, 720, 720])\n",
      "torch.Size([3, 72, 224, 224])\n",
      "torch.Size([3, 151, 720, 720])\n",
      "torch.Size([3, 151, 224, 224])\n",
      "torch.Size([3, 151, 720, 720])\n",
      "torch.Size([3, 151, 224, 224])\n",
      "torch.Size([3, 141, 720, 720])\n",
      "torch.Size([3, 141, 224, 224])\n",
      "torch.Size([3, 141, 720, 720])\n",
      "torch.Size([3, 141, 224, 224])\n",
      "torch.Size([3, 111, 720, 720])\n",
      "torch.Size([3, 111, 224, 224])\n",
      "torch.Size([3, 111, 720, 720])\n",
      "torch.Size([3, 111, 224, 224])\n",
      "torch.Size([3, 177, 720, 720])\n",
      "torch.Size([3, 177, 224, 224])\n",
      "torch.Size([3, 177, 720, 720])\n",
      "torch.Size([3, 177, 224, 224])\n",
      "torch.Size([3, 113, 720, 720])\n",
      "torch.Size([3, 113, 224, 224])\n",
      "torch.Size([3, 113, 720, 720])\n",
      "torch.Size([3, 113, 224, 224])\n",
      "torch.Size([3, 60, 720, 720])\n",
      "torch.Size([3, 60, 224, 224])\n",
      "torch.Size([3, 60, 720, 720])\n",
      "torch.Size([3, 60, 224, 224])\n",
      "torch.Size([3, 144, 720, 720])\n",
      "torch.Size([3, 144, 224, 224])\n",
      "torch.Size([3, 144, 720, 720])\n",
      "torch.Size([3, 144, 224, 224])\n",
      "torch.Size([3, 173, 720, 720])\n",
      "torch.Size([3, 173, 224, 224])\n",
      "torch.Size([3, 173, 720, 720])\n",
      "torch.Size([3, 173, 224, 224])\n",
      "torch.Size([3, 74, 720, 720])\n",
      "torch.Size([3, 74, 224, 224])\n",
      "torch.Size([3, 74, 720, 720])\n",
      "torch.Size([3, 74, 224, 224])\n",
      "torch.Size([3, 423, 720, 720])\n",
      "torch.Size([3, 423, 224, 224])\n",
      "torch.Size([3, 149, 720, 720])\n",
      "torch.Size([3, 149, 224, 224])\n",
      "torch.Size([3, 87, 720, 720])\n",
      "torch.Size([3, 87, 224, 224])\n",
      "torch.Size([3, 273, 720, 720])\n",
      "torch.Size([3, 273, 224, 224])\n",
      "torch.Size([3, 282, 720, 720])\n",
      "torch.Size([3, 282, 224, 224])\n",
      "torch.Size([3, 165, 720, 720])\n",
      "torch.Size([3, 165, 224, 224])\n",
      "torch.Size([3, 137, 720, 720])\n",
      "torch.Size([3, 137, 224, 224])\n",
      "torch.Size([3, 190, 720, 720])\n",
      "torch.Size([3, 190, 224, 224])\n",
      "torch.Size([3, 88, 720, 720])\n",
      "torch.Size([3, 88, 224, 224])\n",
      "torch.Size([3, 110, 720, 720])\n",
      "torch.Size([3, 110, 224, 224])\n",
      "torch.Size([3, 289, 720, 720])\n",
      "torch.Size([3, 289, 224, 224])\n",
      "torch.Size([3, 187, 720, 720])\n",
      "torch.Size([3, 187, 224, 224])\n",
      "torch.Size([3, 206, 720, 720])\n",
      "torch.Size([3, 206, 224, 224])\n",
      "torch.Size([3, 93, 720, 720])\n",
      "torch.Size([3, 93, 224, 224])\n",
      "torch.Size([3, 129, 720, 720])\n",
      "torch.Size([3, 129, 224, 224])\n",
      "torch.Size([3, 346, 720, 720])\n",
      "torch.Size([3, 346, 224, 224])\n",
      "torch.Size([3, 256, 720, 720])\n",
      "torch.Size([3, 256, 224, 224])\n",
      "torch.Size([3, 68, 720, 720])\n",
      "torch.Size([3, 68, 224, 224])\n",
      "torch.Size([3, 113, 720, 720])\n",
      "torch.Size([3, 113, 224, 224])\n",
      "torch.Size([3, 104, 720, 720])\n",
      "torch.Size([3, 104, 224, 224])\n",
      "torch.Size([3, 95, 720, 720])\n",
      "torch.Size([3, 95, 224, 224])\n",
      "torch.Size([3, 51, 720, 720])\n",
      "torch.Size([3, 51, 224, 224])\n",
      "torch.Size([3, 205, 720, 720])\n",
      "torch.Size([3, 205, 224, 224])\n",
      "torch.Size([3, 217, 720, 720])\n",
      "torch.Size([3, 217, 224, 224])\n",
      "torch.Size([3, 109, 720, 720])\n",
      "torch.Size([3, 109, 224, 224])\n",
      "torch.Size([3, 283, 720, 720])\n",
      "torch.Size([3, 283, 224, 224])\n",
      "torch.Size([3, 101, 720, 720])\n",
      "torch.Size([3, 101, 224, 224])\n",
      "torch.Size([3, 62, 720, 720])\n",
      "torch.Size([3, 62, 224, 224])\n",
      "torch.Size([3, 150, 720, 720])\n",
      "torch.Size([3, 150, 224, 224])\n",
      "torch.Size([3, 125, 720, 720])\n",
      "torch.Size([3, 125, 224, 224])\n",
      "torch.Size([3, 35, 720, 720])\n",
      "torch.Size([3, 35, 224, 224])\n",
      "torch.Size([3, 153, 720, 720])\n",
      "torch.Size([3, 153, 224, 224])\n",
      "torch.Size([3, 88, 720, 720])\n",
      "torch.Size([3, 88, 224, 224])\n",
      "torch.Size([3, 143, 720, 720])\n",
      "torch.Size([3, 143, 224, 224])\n",
      "torch.Size([3, 82, 720, 720])\n",
      "torch.Size([3, 82, 224, 224])\n",
      "torch.Size([3, 376, 720, 720])\n",
      "torch.Size([3, 376, 224, 224])\n",
      "torch.Size([3, 396, 720, 720])\n",
      "torch.Size([3, 396, 224, 224])\n",
      "torch.Size([3, 194, 720, 720])\n",
      "torch.Size([3, 194, 224, 224])\n",
      "torch.Size([3, 138, 720, 720])\n",
      "torch.Size([3, 138, 224, 224])\n",
      "torch.Size([3, 481, 720, 720])\n",
      "torch.Size([3, 481, 224, 224])\n",
      "torch.Size([3, 382, 720, 720])\n",
      "torch.Size([3, 382, 224, 224])\n",
      "torch.Size([3, 341, 720, 720])\n",
      "torch.Size([3, 341, 224, 224])\n",
      "torch.Size([3, 328, 720, 720])\n",
      "torch.Size([3, 328, 224, 224])\n",
      "torch.Size([3, 177, 720, 720])\n",
      "torch.Size([3, 177, 224, 224])\n",
      "torch.Size([3, 245, 720, 720])\n",
      "torch.Size([3, 245, 224, 224])\n",
      "torch.Size([3, 876, 720, 720])\n",
      "torch.Size([3, 876, 224, 224])\n",
      "torch.Size([3, 94, 720, 720])\n",
      "torch.Size([3, 94, 224, 224])\n",
      "torch.Size([3, 396, 720, 720])\n",
      "torch.Size([3, 396, 224, 224])\n",
      "torch.Size([3, 215, 720, 720])\n",
      "torch.Size([3, 215, 224, 224])\n",
      "torch.Size([3, 272, 720, 720])\n",
      "torch.Size([3, 272, 224, 224])\n",
      "torch.Size([3, 432, 720, 720])\n",
      "torch.Size([3, 432, 224, 224])\n",
      "torch.Size([3, 596, 720, 720])\n",
      "torch.Size([3, 596, 224, 224])\n",
      "torch.Size([3, 352, 720, 720])\n",
      "torch.Size([3, 352, 224, 224])\n",
      "torch.Size([3, 445, 720, 720])\n",
      "torch.Size([3, 445, 224, 224])\n",
      "torch.Size([3, 573, 720, 720])\n",
      "torch.Size([3, 573, 224, 224])\n",
      "torch.Size([3, 428, 720, 720])\n",
      "torch.Size([3, 428, 224, 224])\n",
      "torch.Size([3, 414, 720, 720])\n",
      "torch.Size([3, 414, 224, 224])\n",
      "torch.Size([3, 341, 720, 720])\n",
      "torch.Size([3, 341, 224, 224])\n",
      "torch.Size([3, 409, 720, 720])\n",
      "torch.Size([3, 409, 224, 224])\n",
      "torch.Size([3, 260, 720, 720])\n",
      "torch.Size([3, 260, 224, 224])\n",
      "torch.Size([3, 847, 720, 720])\n",
      "torch.Size([3, 847, 224, 224])\n",
      "torch.Size([3, 370, 720, 720])\n",
      "torch.Size([3, 370, 224, 224])\n",
      "torch.Size([3, 378, 720, 720])\n",
      "torch.Size([3, 378, 224, 224])\n",
      "torch.Size([3, 618, 720, 720])\n",
      "torch.Size([3, 618, 224, 224])\n",
      "torch.Size([3, 194, 720, 720])\n",
      "torch.Size([3, 194, 224, 224])\n",
      "torch.Size([3, 440, 720, 720])\n",
      "torch.Size([3, 440, 224, 224])\n",
      "torch.Size([3, 465, 720, 720])\n",
      "torch.Size([3, 465, 224, 224])\n",
      "torch.Size([3, 257, 720, 720])\n",
      "torch.Size([3, 257, 224, 224])\n",
      "torch.Size([3, 389, 720, 720])\n",
      "torch.Size([3, 389, 224, 224])\n",
      "torch.Size([3, 75, 720, 720])\n",
      "torch.Size([3, 75, 224, 224])\n",
      "torch.Size([3, 62, 720, 720])\n",
      "torch.Size([3, 62, 224, 224])\n",
      "torch.Size([3, 189, 720, 720])\n",
      "torch.Size([3, 189, 224, 224])\n",
      "torch.Size([3, 97, 720, 720])\n",
      "torch.Size([3, 97, 224, 224])\n",
      "torch.Size([3, 71, 720, 720])\n",
      "torch.Size([3, 71, 224, 224])\n",
      "torch.Size([3, 80, 720, 720])\n",
      "torch.Size([3, 80, 224, 224])\n",
      "torch.Size([3, 92, 720, 720])\n",
      "torch.Size([3, 92, 224, 224])\n",
      "torch.Size([3, 64, 720, 720])\n",
      "torch.Size([3, 64, 224, 224])\n",
      "torch.Size([3, 114, 720, 720])\n",
      "torch.Size([3, 114, 224, 224])\n",
      "torch.Size([3, 128, 720, 720])\n",
      "torch.Size([3, 128, 224, 224])\n",
      "torch.Size([3, 97, 720, 720])\n",
      "torch.Size([3, 97, 224, 224])\n",
      "torch.Size([3, 76, 720, 720])\n",
      "torch.Size([3, 76, 224, 224])\n",
      "torch.Size([3, 78, 720, 720])\n",
      "torch.Size([3, 78, 224, 224])\n",
      "torch.Size([3, 101, 720, 720])\n",
      "torch.Size([3, 101, 224, 224])\n",
      "torch.Size([3, 165, 720, 720])\n",
      "torch.Size([3, 165, 224, 224])\n",
      "torch.Size([3, 119, 720, 720])\n",
      "torch.Size([3, 119, 224, 224])\n",
      "torch.Size([3, 88, 720, 720])\n",
      "torch.Size([3, 88, 224, 224])\n",
      "torch.Size([3, 127, 720, 720])\n",
      "torch.Size([3, 127, 224, 224])\n",
      "torch.Size([3, 167, 720, 720])\n",
      "torch.Size([3, 167, 224, 224])\n",
      "torch.Size([3, 49, 720, 720])\n",
      "torch.Size([3, 49, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# WARNING: This takes up a LOT of space (>100GB). Feel free to skip this step. If you do, just make sure you \n",
    "# use the resize_fn below on your data (I'll have a comment showing where)\n",
    "norm_fn = NormalizeVideo( [0.485, 0.456, 0.406],  [0.229, 0.224, 0.225])\n",
    "resize_fn =  RandomResizedCropVideo(\n",
    "        224, \n",
    "        scale=(1.0,1.0), # use whole image\n",
    "        ratio=(1.0, 1.0) # crop center square\n",
    "    )\n",
    "\n",
    "# This will take a while\n",
    "\n",
    "for video_path in tqdm(glob.glob(path_to_videos + \"\\*\")):\n",
    "    video_root = video_path.split(\"\\\\\")[-1] \n",
    "    video_data, audio_data, metadata = torchvision.io.read_video(video_path)\n",
    "    frames, h, w, c = video_data.shape\n",
    "    video_data = video_data.permute([3,0,1,2]).float()/255 # THWC -> CTHW\n",
    "    video = norm_fn(video_data)\n",
    "    video = resize_fn(video)\n",
    "\n",
    "    new_path = path_to_new_videos + \"\\\\\" + video_root.split('.')[0] + \".pt\"\n",
    "    torch.save(video, new_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1557ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_length = 30\n",
    "class How2SignDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path_to_videos, clip_length):\n",
    "        self.video_root = path_to_videos\n",
    "        self.clip_length = clip_length\n",
    "        \n",
    "        # Get list of paths to all videos\n",
    "        list_of_all_videos = glob.glob(path_to_videos+\"\\*\")\n",
    "        \n",
    "        # Get their corresponding labels\n",
    "        self.list_of_all_video_ids = [x.split(\"//\")[-1].split(\"_\")[0] for x in list_of_all_videos]\n",
    "        self.unique_video_ids = set(self.list_of_all_video_ids)\n",
    "        # Calculate how many clips you have\n",
    "        # Recall that the number of N frames clips in an M frame video: # = M-N+1\n",
    "        # N = clip_length\n",
    "        # M = number of frames in a video\n",
    "        \n",
    "        # We want to sum across all videos\n",
    "        # What happens if M<N? \n",
    "            # You can choose how to handle this. I recommend 0-padding in the __getitem__ function.\n",
    "            # More information on how to do this in the __getitem__ function\n",
    "        total_clips = 0\n",
    "        for video in list_of_all_videos:\n",
    "            # TODO \n",
    "            pass\n",
    "        self.length = total_clips\n",
    "        \n",
    "        # TODO:\n",
    "        # This is not trivial, think through the process\n",
    "        # When we retrieve some index with the __getitem__, we need to know which video path and label the index corresponds to\n",
    "        # This is not equal to the index of the video/label, since one video might have many examples\n",
    "        \n",
    "        # Additionally, we need our index to correspond to a unique slice of that video. \n",
    "        # You need to add some method of keeping track of this\n",
    "        \n",
    "        # I recommend creating a list with self.length entries\n",
    "        # Each entry is a tuple of (video_path, label, starting_frame)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.length)\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the correct clip and label\n",
    "        video, label = None, None\n",
    "        \n",
    "        # If you didn't process videos, you should process your videos here:\n",
    "        \n",
    "        # These can go in the __init__ \n",
    "        # self.norm_fn = NormalizeVideo( [0.485, 0.456, 0.406],  [0.229, 0.224, 0.225])\n",
    "        # self.resize_fn = RandomResizedCropVideo(\n",
    "        #         224, \n",
    "        #         scale=(1.0,1.0), # use whole image\n",
    "        #         ratio=(1.0, 1.0) # crop center square\n",
    "        #     )\n",
    "        \n",
    "        # These should go here\n",
    "        # video_data = video_data.permute([3,0,1,2]).float()/255\n",
    "        # video_data = self.resize_fn(self.norm_fn(video_data))\n",
    "        \n",
    "        # What happens if M<N? \n",
    "            # 1. Check if this is the case (if the number of frames in the video < self.clip_length)\n",
    "            # If True, one solution is to pad the video to end with 0s \n",
    "                # video = torch.zeros(3, self.clip_length, 224, 224)\n",
    "                # video[:, :num_frames, :, :] = video_data\n",
    "            # if False, no problem\n",
    "                # video = video_data[:, start_frame:start_frame + self.clip_length]\n",
    "        \n",
    "        # You should return video, label such that:\n",
    "        # video.shape = C, T, H, W = 3, self.clip_length, 224, 224\n",
    "        # label should be a single integer representing the INDEX that the label corresponds to \n",
    "                # (use self.list_of_all_video_ids.index('string'))\n",
    "            \n",
    "        \n",
    "        return video, label\n",
    "        \n",
    "dataset = How2SignDataset(path)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f6bfb7",
   "metadata": {},
   "source": [
    "## Next: Create a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d14b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Layers:\n",
    "\n",
    "# Conv3D \n",
    "# MaxPool3D (if you only want to reduce spatial resolution and not temporal, use a kernel with width 1 in time dim)\n",
    "    # torch.nn.MaxPool3d((2,2,2)) # takes the max of a 2x2x2 area. Shape BCTHW becomes BC(T/2)(H/2)(W/2)\n",
    "    # torch.nn.MaxPool3d((1,3,2)) # Takes the max of a 1x3x2 area. Shape BCTHW becomes BCT(H/3)(W/2)\n",
    "# BatchNorm3D\n",
    "    # Normalizes our data with mean 0 standard deviation 1. \n",
    "    # Helps with learning, reduces 'internal covariate shift'\n",
    "    # If I forgot to talk about this remind me\n",
    "# ReLU\n",
    "    # Or some other non-linearity\n",
    "    \n",
    "# Recommended design\n",
    "    # Use 5 'stages', where each stage ends with a maxpool\n",
    "    # Inside each stage, repeat a 'Convolutional Block' N times.\n",
    "        # A Convolutional Block generally consists of: Conv, BatchNorm, ReLU or Conv, ReLU, BatchNorm\n",
    "    # Generally, each stage will have the same number of channels\n",
    "    \n",
    "# Visual Explaination:\n",
    "    # Stage1\n",
    "        # Block1\n",
    "            # Conv, 32 channels\n",
    "            # ReLU\n",
    "            # BatchNorm\n",
    "    # Maxpool\n",
    "    # Stage 2\n",
    "        # Block1\n",
    "            # Conv, 64 channels\n",
    "            # ReLU\n",
    "            # BatchNorm\n",
    "        # Block2\n",
    "            # Conv, 64 channels\n",
    "            # ReLU\n",
    "            # BatchNorm\n",
    "    # ... \n",
    "    \n",
    "# At the end of your model:\n",
    "    # 1. Flatten your data\n",
    "        # B, C, T, H, W -> B, C*T*H*W \n",
    "        # Use .flatten(start_dim=1)\n",
    "    # 2. Use Linear layer(s)\n",
    "        # torch.nn.Linear(input_features, output_features)\n",
    "        # i.e. self.fc1= nn.Linear(C*T*H*W, hidden_dim1)\n",
    "        # ... self.fcN = nn.Linear(hidden_dimN, num_classes)\n",
    "    # 3. Don't forget to ReLU between stages\n",
    "    # 4. Dropout is also useful\n",
    "    # 5. Finally, make sure to Softmax before returning \n",
    "         # so that each entry is a probability of that class\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb62250",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=None):\n",
    "        super(VideoClassifier, self).__init__()\n",
    "        # TODO add layers\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # X.shape = B, C, T, H, W = B, 3, num\n",
    "        # TODO add layers\n",
    "        return x\n",
    "    \n",
    "model = VideoClassifier(num_classes=len(dataset.unique_video_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d98a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick test of the model\n",
    "model(torch.rand(4, 3, clip_length, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9b6df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model summary\n",
    "summary(model, (3, clip_length, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23801ba5",
   "metadata": {},
   "source": [
    "## Next, train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70e2023",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = # TODO choose batch size\n",
    "lr = # TODO choose learning rate\n",
    "loss_fn = # TODO (which loss should we use for multilass classification?)\n",
    "opt = # TODO choose an optimizer: https://pytorch.org/docs/stable/optim.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc8fe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660b7675",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1): # TODO what is an epoch?\n",
    "    for x, y in train_dataloader:\n",
    "        # TODO zero gradients\n",
    "        # TODO pass input through model (perform forward pass)\n",
    "        # TODO calculate loss\n",
    "        # TODO calculate gradients (perform backward pass)\n",
    "        # TODO apply gradients (with your optimizer)\n",
    "        break # we'll just do one sample since they take a while\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
